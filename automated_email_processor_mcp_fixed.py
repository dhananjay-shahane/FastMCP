#!/usr/bin/env python3
"""
Automated Email Processor with MCP Resources, Prompts and LLM ONLY

This MCP service:
1. Uses @mcp.resource() for file access
2. Uses @mcp.prompt() for LLM interactions  
3. Uses only LLM for all responses - NO fallback code
4. All messages generated by Ollama LLM
"""

import asyncio
import logging
import sys
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import re

try:
    from fastmcp import FastMCP, Context
except ImportError:
    print("Error: FastMCP not available. Install with: pip install fastmcp")
    sys.exit(1)

# Import MCP server components
from email_enhanced_server import email_handler, ollama_llm, EMAIL_CONFIG

# Initialize FastMCP with resources and prompts
mcp = FastMCP("AutomatedEmailProcessor")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger('mcp_email_processor')

# MCP Resources for file access
@mcp.resource("file://data/{filename}")
async def read_data_file(filename: str) -> str:
    """Read CSV files from data directory via MCP resource"""
    try:
        data_path = Path(__file__).parent / "data" / filename
        if not data_path.exists():
            raise FileNotFoundError(f"Data file {filename} not found")
        
        if not filename.endswith('.csv'):
            raise ValueError(f"File {filename} is not a CSV file")
        
        with open(data_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        logger.info(f"üìÑ Successfully read data file: {filename}")
        return content
        
    except Exception as e:
        logger.error(f"Error reading data file {filename}: {str(e)}")
        raise

@mcp.resource("file://scripts/{filename}")
async def read_script_file(filename: str) -> str:
    """Read Python scripts from scripts directory via MCP resource"""
    try:
        script_path = Path(__file__).parent / "scripts" / filename
        if not script_path.exists():
            raise FileNotFoundError(f"Script file {filename} not found")
        
        if not filename.endswith('.py'):
            raise ValueError(f"File {filename} is not a Python script")
        
        with open(script_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        logger.info(f"üìú Successfully read script file: {filename}")
        return content
        
    except Exception as e:
        logger.error(f"Error reading script file {filename}: {str(e)}")
        raise

# MCP Prompts for LLM interactions
@mcp.prompt("analyze_email_intent")
async def analyze_email_intent_prompt(email_body: str, sender: str, subject: str, available_files: str) -> str:
    """Generate prompt for analyzing email intent with LLM"""
    return f"""Analyze this email and determine the user's intent:

Email Details:
- From: {sender}
- Subject: {subject}
- Body: {email_body}

Available Resources:
{available_files}

Determine:
1. Is this a data analysis/visualization request?
2. What type of visualization (bar, line, pie, stats)?
3. Which data file should be used?
4. What specific analysis is needed?
5. Does the request need clarification?

Respond with your analysis."""

@mcp.prompt("generate_email_response")  
async def generate_email_response_prompt(context: str, analysis: str, execution_result: str = "") -> str:
    """Generate prompt for creating email responses with LLM"""
    return f"""Generate a professional email response based on this context:

{context}

Analysis performed: {analysis}
Execution result: {execution_result}

Create a helpful, professional email response. If data analysis was performed, mention the results and attachments. If clarification is needed, ask specific questions about data files and visualization types."""

def extract_email_address(from_field: str) -> Optional[str]:
    """Extract email address from From field"""
    try:
        email_match = re.search(r'[\w\.-]+@[\w\.-]+\.\w+', from_field)
        return email_match.group(0) if email_match else None
    except Exception:
        return None

@mcp.tool()
async def get_available_files_mcp(ctx: Context | None = None) -> Dict[str, Any]:
    """Get available data and script files via MCP resources"""
    try:
        csv_files = []
        script_files = []
        
        # Get CSV files from data directory using MCP resource pattern
        data_dir = Path(__file__).parent / "data"
        for csv_file in data_dir.glob('*.csv'):
            try:
                # Test if file is accessible via MCP resource
                await read_data_file(csv_file.name)
                csv_files.append({
                    'name': csv_file.name,
                    'size': csv_file.stat().st_size,
                    'available': True
                })
            except Exception:
                csv_files.append({
                    'name': csv_file.name,
                    'available': False
                })
        
        # Get script files from scripts directory using MCP resource pattern
        scripts_dir = Path(__file__).parent / "scripts"
        for script_file in scripts_dir.glob('*.py'):
            try:
                # Test if file is accessible via MCP resource
                await read_script_file(script_file.name)
                script_files.append(script_file.name)
            except Exception:
                pass
        
        return {
            "success": True,
            "csv_files": csv_files,
            "script_files": script_files
        }
        
    except Exception as e:
        logger.error(f"Error getting files via MCP: {str(e)}")
        return {"success": False, "error": str(e)}

def analyze_llm_response(response_text: str, available_files: Dict[str, Any]) -> Dict[str, Any]:
    """Analyze LLM text response to extract intent"""
    response_lower = response_text.lower()
    
    # Check for visualization keywords
    viz_keywords = {
        'bar': 'bar_chart',
        'column': 'bar_chart',
        'line': 'line_graph',
        'trend': 'line_graph',
        'pie': 'pie_chart',
        'donut': 'pie_chart',
        'stats': 'custom_stats',
        'statistics': 'custom_stats'
    }
    
    visualization_type = None
    for keyword, viz_type in viz_keywords.items():
        if keyword in response_lower:
            visualization_type = viz_type
            break
    
    # Check for data file mentions
    data_file = None
    for file_info in available_files.get('csv_files', []):
        if file_info['name'].lower() in response_lower:
            data_file = file_info['name']
            break
    
    # Determine if this is a data request
    data_keywords = ['chart', 'graph', 'plot', 'visualization', 'analyze', 'data']
    is_data_request = any(keyword in response_lower for keyword in data_keywords)
    
    return {
        'requires_data_processing': is_data_request and visualization_type and data_file,
        'visualization_type': visualization_type,
        'data_file': data_file,
        'needs_clarification': is_data_request and (not visualization_type or not data_file),
        'intent': 'data_analysis' if is_data_request else 'general_inquiry'
    }

async def execute_data_analysis(analysis: Dict[str, Any]) -> Dict[str, Any]:
    """Execute data analysis script directly"""
    try:
        script_name = f"{analysis.get('visualization_type', 'bar_chart')}.py"
        data_file = analysis.get('data_file')
        
        if not data_file:
            return {"success": False, "error": "No data file specified"}
        
        script_path = Path(__file__).parent / "scripts" / script_name
        data_path = Path(__file__).parent / "data" / data_file
        
        if not script_path.exists():
            return {"success": False, "error": f"Script {script_name} not found"}
        
        if not data_path.exists():
            return {"success": False, "error": f"Data file {data_file} not found"}
        
        # Execute script
        result = subprocess.run(
            [sys.executable, str(script_path), str(data_path)],
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if result.returncode == 0:
            # Find generated output files
            output_dir = Path(__file__).parent / "output"
            timestamp = datetime.now().strftime('%Y%m%d')
            generated_files = list(output_dir.glob(f'*{timestamp}*.png'))
            
            return {
                "success": True,
                "script_output": result.stdout,
                "generated_files": [str(f) for f in generated_files],
                "script_name": script_name,
                "data_file": data_file
            }
        else:
            return {
                "success": False,
                "error": result.stderr,
                "script_output": result.stdout
            }
        
    except Exception as e:
        logger.error(f"Error executing analysis: {str(e)}")
        return {"success": False, "error": str(e)}

async def process_email_with_llm_only(email_data: Dict[str, Any]) -> Dict[str, Any]:
    """Process email using ONLY LLM and direct execution - NO fallback responses"""
    try:
        sender = email_data.get('from', '')
        subject = email_data.get('subject', '')
        body = email_data.get('body', '')
        
        logger.info(f"üìß Processing email: {subject[:50]}...")
        logger.info(f"From: {sender}")
        
        # Extract sender email
        sender_email = extract_email_address(sender)
        if not sender_email:
            logger.warning(f"Could not extract sender email from: {sender}")
            return {"success": False, "error": "Invalid sender email"}
        
        # Get available files via MCP tool
        available_files = await get_available_files_mcp()
        
        # Generate analysis prompt
        files_summary = f"CSV files: {[f['name'] for f in available_files['csv_files']]}\nScript files: {available_files['script_files']}"
        analysis_prompt = f"""Analyze this email and determine the user's intent:

Email Details:
- From: {sender_email}
- Subject: {subject}
- Body: {body}

Available Resources:
{files_summary}

Determine:
1. Is this a data analysis/visualization request?
2. What type of visualization (bar, line, pie, stats)?
3. Which data file should be used?
4. What specific analysis is needed?
5. Does the request need clarification?

Respond with your analysis."""
        
        # Get LLM analysis with shorter timeout to avoid context cancellation
        llm_analysis_response = await ollama_llm.generate_response(analysis_prompt, "Analyze email intent:", timeout=15)
        analysis = analyze_llm_response(llm_analysis_response, available_files)
        
        logger.info(f"ü§ñ LLM Analysis: {analysis}")
        
        # Process based on analysis
        if analysis.get('requires_data_processing', False):
            logger.info(f"‚öôÔ∏è Executing data analysis: {analysis['visualization_type']} on {analysis['data_file']}")
            
            # Execute data analysis
            execution_result = await execute_data_analysis(analysis)
            
            # Generate response with analysis results
            response_prompt = f"""Generate a professional email response based on this context:

Original request: {body}
From: {sender_email}
Subject: {subject}

Analysis performed: Data file: {analysis.get('data_file')}, Visualization: {analysis.get('visualization_type')}
Execution result: {str(execution_result)}

Create a helpful, professional email response. Mention the data analysis performed and any attached files."""
            
            attachments = execution_result.get('generated_files', [])
        else:
            # Generate response for clarification or general inquiry  
            response_prompt = f"""Generate a professional email response based on this context:

Original request: {body}
From: {sender_email}
Subject: {subject}

Analysis performed: Intent: {analysis.get('intent')}, Needs clarification: {analysis.get('needs_clarification')}
Available files: {available_files}

Create a helpful, professional email response. If clarification is needed, ask specific questions about data files and visualization types."""
            
            attachments = []
        
        # Generate email response using LLM with shorter timeout  
        llm_response_content = await ollama_llm.generate_response(response_prompt, "Generate professional email response:", timeout=10)
        
        # If LLM response is empty, handle gracefully
        if not llm_response_content:
            logger.warning("‚ö†Ô∏è Empty LLM response, using analysis fallback")
            if analysis.get('requires_data_processing'):
                llm_response_content = f"I've completed your data analysis request using {analysis.get('data_file')} and generated a {analysis.get('visualization_type')} visualization. Please see the attached file."
            else:
                llm_response_content = f"Thank you for your email. I can help with data visualization from CSV files. Available files: {[f['name'] for f in available_files.get('csv_files', [])]}"
        
        logger.info(f"üìù Generated response length: {len(llm_response_content)} characters")
        
        # Send email using email handler
        response_subject = f"Re: {subject}" if not subject.startswith('Re:') else subject
        success = await email_handler.send_email(
            to_email=sender_email,
            subject=response_subject,
            body=llm_response_content,
            attachments=attachments
        )
        
        if success:
            logger.info(f"‚úÖ Email sent successfully to {sender_email}")
            if attachments:
                logger.info(f"üìé Included {len(attachments)} attachments")
        else:
            logger.error(f"‚ùå Failed to send email to {sender_email}")
        
        return {
            "success": success,
            "analysis": analysis,
            "response_content": llm_response_content,
            "attachments": attachments,
            "sender_email": sender_email
        }
        
    except Exception as e:
        logger.error(f"Error in LLM-only email processing: {str(e)}")
        return {"success": False, "error": str(e)}

async def check_and_process_emails() -> Dict[str, Any]:
    """Check and process emails using LLM only"""
    try:
        logger.info("üì® Checking for new emails...")
        
        # Get emails via email handler
        emails = await email_handler.get_filtered_emails(limit=5)
        
        if not emails:
            return {"success": True, "emails_processed": 0, "message": "No new emails"}
        
        processed_emails = []
        
        # Process each email using LLM only
        for email_data in emails:
            result = await process_email_with_llm_only(email_data)
            processed_emails.append(result)
            if result.get('success'):
                logger.info(f"‚úÖ Successfully processed email: {email_data.get('subject', '')[:50]}...")
            else:
                logger.error(f"‚ùå Failed to process email: {email_data.get('subject', '')[:50]}...")
            
        return {
            "success": True,
            "emails_processed": len(processed_emails),
            "results": processed_emails
        }
        
    except Exception as e:
        logger.error(f"Error checking emails: {str(e)}")
        return {"success": False, "error": str(e)}

async def start_llm_only_monitoring():
    """Start automated email monitoring using ONLY LLM and direct execution"""
    logger.info("üöÄ Starting LLM-ONLY email processing workflow...")
    logger.info(f"üìß Monitoring emails from: {EMAIL_CONFIG['allowed_sender']}")
    logger.info(f"‚è∞ Check interval: 30 seconds")
    logger.info("ü§ñ ALL responses generated by LLM - NO fallback code")
    logger.info("üîß Direct file operations and script execution")
    logger.info("Press Ctrl+C to stop...")
    
    try:
        processed_count = 0
        while True:
            check_result = await check_and_process_emails()
            if check_result.get('emails_processed', 0) > 0:
                processed_count += check_result['emails_processed']
                logger.info(f"üìä Total emails processed: {processed_count}")
            
            await asyncio.sleep(30)
            
    except KeyboardInterrupt:
        logger.info("\n‚èπÔ∏è  LLM email monitoring stopped by user")
        return {"success": True, "message": "Monitoring stopped", "processed_count": processed_count}
    except Exception as e:
        logger.error(f"Error in LLM monitoring: {str(e)}")
        return {"success": False, "error": str(e)}

async def main():
    """Main function to run LLM-only automated email processor"""
    try:
        await start_llm_only_monitoring()
    except Exception as e:
        logger.error(f"Fatal error in LLM email processor: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())